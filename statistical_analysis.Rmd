---
title: "Statistical Analysis"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(glmnet)
library(tidymodels)
library(performance)
library(olsrr)
library(kableExtra)

outcome_by_year <- 
  read_csv("./data/outcome_puma_by_year.csv") %>%
  rename(puma = puma10)

unbiased_data <- read_csv("./data/unbiased_group_means.csv") %>%
  merge(outcome_by_year, by = "puma")
```

## Regression Methodology

```{r, echo = TRUE, message = FALSE, warning = FALSE}
best_model <- lm(puma_hosp_rate_2020  ~ language_spanish + 
                   education_bachelors_degree +
                   birthplace_us + health_insurance_public + language_english + 
                   personal_income + employment_not_in_labor_force +
                   health_insurance_public:personal_income +
                   education_bachelors_degree:personal_income +
                   language_english:birthplace_us, data = unbiased_data)

full_model <-  lm(puma_hosp_rate_2020  ~ 
                   (language_spanish + 
                   education_bachelors_degree +
                   birthplace_us + health_insurance_public + language_english + 
                   personal_income + employment_not_in_labor_force)^2, 
                  data = unbiased_data)
```

Let $S$ denote the set of PUMAs in NYC. We consider the latent variable model
	$$ y_s = \xi_s' \beta + \varepsilon_s $$
for each $s \in S$, where $\xi_s$ a vector of PUMA-level means of data from the census. There are two challenges in considering such a model: (1) we know historically that NYC neighborhoods, particularly those within boroughs, are subject to spatial correlation; and (2) because the census data is recorded at the interview level, we do not observe the group-level means $\xi_s$.

In order to address (1), we employ heteroscedasticity-robust White standard errors to deal with the potential threat of spatial correlation. We find that all of our coefficient estimates retain their significance under the adjusted standard errors, and hence we leave this check on robustness to the appendix.

More interesting is the challenge presented in (2). For each $s \in S$, suppose we observe $i \in \{ 1, \ldots, n_s \}$ individual-level interviews. It is natural to consider the group mean
	$$ \bar{X}_s \equiv \frac{1}{n_s} \sum_{i = 1}^{n_s} X_{is}. $$
However, as shown in Croon and Veldhoven (2007), it is generally the case that using the observed group mean $\bar{X}_s$ as an estimator of $\xi_s$ will lead to biased regression coefficients. Thus, we address (2) by following the procedure outlined by Croon and Veldhoven to re-weight our group means and obtain adjusted group means $\tilde{X}_s$. In particular, let $ \hat{\Sigma}_{\xi \xi}$ and $\hat{\Sigma}_{\nu \nu}$ denote the usual ANOVA estimates of the between- and within-group variation matrices, respectively. The adjusted group means are then given by
	$$ \tilde{X}_s \equiv \bar{X}' (I - W_s) + \bar{X}_s' W_s, \quad \text{where} $$
	$$ W_s \equiv \left( \hat{\Sigma}_{\xi \xi} + \hat{\Sigma}_{\nu \nu} / n_s \right)^{-1} \hat{\Sigma}_{\xi \xi} $$
and $I$ denotes the identity matrix.

## Regression Dianostics
```{r echo = TRUE, fig.height=6.2, fig.width= 8, message=FALSE, warning=FALSE}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

best_model_tidy <- fit(lm_spec, puma_hosp_rate_2020 ~ language_spanish + 
                   education_bachelors_degree +
                   birthplace_us + health_insurance_public + language_english + 
                   personal_income + employment_not_in_labor_force +
                   health_insurance_public:personal_income +
                   education_bachelors_degree:personal_income +
                   language_english:birthplace_us, data = unbiased_data)


check_model(best_model_tidy, 
            check = c("linearity", "homogeneity","qq","normality", "outliers"))



```

Linearity:
It is to be observed that the relationship between independent and dependent  variables is fairly linear in our model. Thus,  we claim that the mean of the hospitalization rate in 2020 (outcome) is a linear combination of the model parameters and the predictor variables. 

Equal Variance (homoscedasticity): 
Based on the residual vs fitted plot, it is to be observed that the points in the plot are randomly dispersed around the horizontal line at 0 without showing a certain trend. This implies no violation of homoscedasticity. 

Normality of residuals: 
According to the normality probability plot, it is to be observed most of the points fall along the line. Therefore, we claim that residuals are normally distributed in our model. 

Influential points among outliers: 
According to residuals vs leverage plot, it is to be observed all points are inside the contour lines. Hence, there are no influential observations. Therefore, we claim that the residuals are independent of each other. 

Residuals are uncorrelated: 
On the basis of the autocorrelation plot, it is to be observed that the lags do not have significant effect because they are within the bounds. 


```{r, echo = TRUE, message = FALSE, warning = FALSE}
model_summary <-
  summary(best_model) %>% 
  broom::glance() %>%
  bind_rows(summary(full_model) %>% broom::glance()) %>%
  mutate(model = c("Best Model", "Full Model")) %>%
  relocate(model)

Cp <- ols_mallows_cp(best_model, full_model)

Cp <- as.tibble(Cp)

names(Cp) <- c("Mallows' Cp")

kbl(model_summary) %>%
  kable_paper("striped", full_width = F) %>%
  column_spec(1:6, bold = T) %>%
  row_spec(1:2, bold = T, color = "white", background = "black")

kbl(Cp) %>%
  kable_paper("striped", full_width = F) %>%
  column_spec(1, bold = T) %>%
  row_spec(1, bold = T, color = "white", background = "black")
```

Adjusted R-squared: 
We compared our model to the full model which includes all main effects of our model with all possible second-order interaction terms of main effects. It is obvious that the full model has higher R-squared (0.797) than that of our model (0.669). However, our model has a higher adjusted R-squared (0.593) than that of the full model (0.579).

Mallows’ Cp: 
Based on Mallows’ Cp criterion, we want our model to have Cp value less than or equal to the number of parameters, indicating little or no bias in the regression model. we obtained our Cp) from our model and the full model.  The Cp value is 9.5 and it is less than the number of our model parameters (11).  Therefore, we claim that our regression model has little or no bias.


## Model Summary
```{r, echo = TRUE, message = FALSE, warning = FALSE}
summary <-
  summary(best_model) %>%
  broom::tidy() 

kbl(summary) %>%
  kable_paper("striped", full_width = F) %>%
  column_spec(1:5, bold = T) %>%
  row_spec(1:11, bold = T, color = "white", background = "black")
```
